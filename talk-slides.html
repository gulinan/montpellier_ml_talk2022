<!DOCTYPE html>
<html lang="" xml:lang="">
  <head>
    <title>Cross-validation: What does it estimate and how well does it?</title>
    <meta charset="utf-8" />
    <meta name="author" content="Gül İnan  " />
    <script src="libs/header-attrs-2.11/header-attrs.js"></script>
    <link href="libs/remark-css-0.0.1/metropolis.css" rel="stylesheet" />
    <script src="libs/fabric-4.3.1/fabric.min.js"></script>
    <link href="libs/xaringanExtra-scribble-0.0.1/scribble.css" rel="stylesheet" />
    <script src="libs/xaringanExtra-scribble-0.0.1/scribble.js"></script>
    <script>document.addEventListener('DOMContentLoaded', function() { window.xeScribble = new Scribble({"pen_color":["#1A292C"],"pen_size":3,"eraser_size":30,"palette":[]}) })</script>
    <link href="libs/panelset-0.2.6/panelset.css" rel="stylesheet" />
    <script src="libs/panelset-0.2.6/panelset.js"></script>
    <link rel="stylesheet" href="cal.css" type="text/css" />
  </head>
  <body>
    <textarea id="source">
class: center, middle, inverse, title-slide

# <b>Cross-validation: What does it estimate and how well does it?</b>
## by Stephen Bates, Trevor Hastie, and Robert Tibshirani <br>
### Gül İnan <br>
### <br>Istanbul Technical University
### <br><i>Machine Learning in Montpellier, Theory &amp; Practice, <br> January 6th, 2022 </i>

---






# Motivation
&lt;br&gt;
- In machine learning applications, when deploying a &lt;b&gt;&lt;span style="color:#067373;"&gt;predictive model&lt;/span&gt;&lt;/b&gt;, the main interest is on understanding &lt;b&gt;&lt;span style="color:#067373;"&gt;prediction accuracy&lt;/span&gt;&lt;/b&gt; of the model on future test points. 
- The standard measure of accuracy for a predictive model is the &lt;b&gt;&lt;span style="color:#067373;"&gt;prediction error&lt;/span&gt;&lt;/b&gt;, i.e., the &lt;b&gt;&lt;span style="color:#067373;"&gt;expected loss on future test points&lt;/span&gt;&lt;/b&gt;.
- For inference, both &lt;b&gt;&lt;span style="color:#067373;"&gt;good point estimates&lt;/span&gt;&lt;/b&gt; and &lt;b&gt;&lt;span style="color:#067373;"&gt;accurate confidence intervals&lt;/span&gt;&lt;/b&gt; are required for &lt;b&gt;&lt;span style="color:#067373;"&gt;prediction error&lt;/span&gt;&lt;/b&gt;.

---
# Motivation continu'ed
&lt;br&gt;
- From a practical point of view, &lt;b&gt;&lt;span style="color:#067373;"&gt;cross-validation (CV)&lt;/span&gt;&lt;/b&gt; is one of the widely-used resampling-based approaches to estimate a &lt;b&gt;&lt;span style="color:#067373;"&gt;point estimate&lt;/span&gt;&lt;/b&gt; and to build &lt;b&gt;&lt;span style="color:#067373;"&gt;confidence intervals&lt;/span&gt;&lt;/b&gt; for prediction error.
- However, &lt;b&gt;&lt;span style="color:#067373;"&gt;Bates, Hastie, and Tibshirani (2021)&lt;/span&gt;&lt;/b&gt; discuss that (statistical) properties of CV estimator of prediction error are **not well-understood** despite CV has a very simple functionality.

---
# Aim
&lt;br&gt;
- &lt;b&gt;&lt;span style="color:#067373;"&gt;Bates, Hastie, and Tibshirani (2021)&lt;/span&gt;&lt;/b&gt; firstly show that the &lt;b&gt;&lt;span style="color:#067373;"&gt;CV estimator of prediction error&lt;/span&gt;&lt;/b&gt;:
  - tracks **the accuracy of the model fit weakly** and, instead
  - estimates **the average prediction error of models fit across many (hypothetical) data sets from the same population**.  

---
# Aim continu'ed
&lt;br&gt; 
- &lt;b&gt;&lt;span style="color:#067373;"&gt;Bates, Hastie, and Tibshirani (2021)&lt;/span&gt;&lt;/b&gt; secondly show that &lt;/span&gt;&lt;/b&gt; the &lt;b&gt;&lt;span style="color:#067373;"&gt;naive confidence intervals&lt;/span&gt;&lt;/b&gt; based on CV estimate of prediction error give **poor coverage**, which are far below nominal level since the estimation of variance used to compute the width of the interval does not account for the **correlation between error estimates** in different folds, which arises  because each data point is used for both training and testing.  
- &lt;b&gt;&lt;span style="color:#067373;"&gt;Bates, Hastie, and Tibshirani (2021)&lt;/span&gt;&lt;/b&gt; propose the &lt;b&gt;&lt;span style="color:#067373;"&gt;nested cross-validation (NCV)&lt;/span&gt;&lt;/b&gt; approach which delivers confidence intervals with a **coverage close to the nominal level**.  
- &lt;b&gt;&lt;span style="color:#067373;"&gt;Bates, Hastie, and Tibshirani (2021)&lt;/span&gt;&lt;/b&gt; validate their work with theory and extensive numerical experiments (both simulation studies and real data examples).
 
---
# Setting and notation
&lt;br&gt;
- Consider a &lt;b&gt;&lt;span style="color:#067373;"&gt;supervised learning &lt;/span&gt;&lt;/b&gt; setting.  
- We have &lt;b&gt;&lt;span style="color:#067373;"&gt;features&lt;/span&gt;&lt;/b&gt; `\(X=(X_1, \ldots X_n)  \in \mathcal{X}^{n}\)`
and &lt;b&gt;&lt;span style="color:#067373;"&gt;responses&lt;/span&gt;&lt;/b&gt; `\(Y=(Y_1, \ldots, Y_n)  \in \mathcal{Y}^{n}\)`.
- We assume that each &lt;b&gt;&lt;span style="color:#067373;"&gt;data point&lt;/span&gt;&lt;/b&gt; `\((X_i,Y_i)\)` for `\(i=1,\ldots,n\)`
are i.i.d. from a &lt;b&gt;&lt;span style="color:#067373;"&gt; distribution&lt;/span&gt;&lt;/b&gt; `\(P\)`. 



---
# Setting and notation continu'ed
&lt;br&gt;
- Consider a &lt;b&gt;&lt;span style="color:#067373;"&gt;class of models &lt;/span&gt;&lt;/b&gt; parameterized by `\(\theta\)`.
- We let `\(\widehat f(x, \theta)\)` be the &lt;b&gt;&lt;span style="color:#067373;"&gt;function that predicts&lt;/span&gt;&lt;/b&gt; `\(y\)` from `\(x \in \mathbb{R}^{p}\)`, where `\(\theta\)` takes values in some space `\(\Theta\)`.
- We let `\(\mathcal{A}\)` be a &lt;b&gt;&lt;span style="color:#067373;"&gt;model-fitting algorithm&lt;/span&gt;&lt;/b&gt; that takes data points and returns a parameter vector `\(\widehat \theta \in \Theta\)`. 
- We let `\(\hat \theta =\mathcal{A}(X,Y)\)` be the &lt;b&gt;&lt;span style="color:#067373;"&gt; fitted value of the parameter &lt;/span&gt;&lt;/b&gt; based on the observed data `\(X\)` and `\(Y\)`. 

---
# Prediction error 
&lt;br&gt;
- In &lt;b&gt;&lt;span style="color:#067373;"&gt;measuring accuracy of a model&lt;/span&gt;&lt;/b&gt;, we are interested in &lt;b&gt;&lt;span style="color:#067373;"&gt;out-of-sample error&lt;/span&gt;&lt;/b&gt; which is defined as the &lt;b&gt;&lt;span style="color:#067373;"&gt;expected loss on unseen data points&lt;/span&gt;&lt;/b&gt; :

`\begin{equation}
Err_{XY}: = \mathbb{E}\bigg[	\ell \big(\hat{f}(X_{n+1},\hat \theta ),Y_{n+1}\big) \vert (X,Y) \bigg] , \nonumber
\end{equation}`

- where `\((X_{n+1},Y_{n+1})\)` is an &lt;b&gt;&lt;span style="color:#067373;"&gt;independent test point&lt;/span&gt;&lt;/b&gt; from the same distribution `\(P\)`. 
- The expression `\(\hat{f}(X_{n+1},\hat \theta)\)` is the &lt;b&gt;&lt;span style="color:#067373;"&gt;predicted value&lt;/span&gt;&lt;/b&gt; of `\(Y_{n+1}\)` at the future point `\(X_{n+1}\)` and `\(\hat \theta\)` is the fitted value of the parameter estimated through algorithm `\(\mathcal{A}\)` based on the training data `\((X,Y)\)`.
- The expression `\(\ell \big(\hat{f}(X_{n+1},\hat \theta ),Y_{n+1}\big)\)` is the &lt;b&gt;&lt;span style="color:#067373;"&gt;loss&lt;/span&gt;&lt;/b&gt; between predicted value of `\(Y_{n+1}\)` and `\(Y_{n+1}\)` itself.
- Here, the &lt;b&gt;&lt;span style="color:#067373;"&gt;loss function&lt;/span&gt;&lt;/b&gt; `\(\ell(.)\)` could be squared error loss, classification error, or deviance (cross-entropy). 
- Furthermore `\(Err_{XY}\)` can be considered as a &lt;b&gt;&lt;span style="color:#067373;"&gt;random quantity&lt;/span&gt;&lt;/b&gt;  depending on the training data `\((X,Y)\)`.

---
# Expected prediction error
&lt;br&gt;
- On the other hand, in designing and comparing &lt;b&gt;&lt;span style="color:#067373;"&gt;learning algorithms&lt;/span&gt;&lt;/b&gt; themselves, we are interested in their &lt;b&gt;&lt;span style="color:#067373;"&gt;average performance&lt;/span&gt;&lt;/b&gt; on predicting future test points.
- We can formally define this quantity as
the &lt;b&gt;&lt;span style="color:#067373;"&gt;expected value of prediction error when training over possible data sets of size n drawn from the same data distribution `\(P\)`&lt;/span&gt;&lt;/b&gt;:

`\begin{equation}
Err :=\mathbb{E}\big[Err_{XY}\big].\nonumber
\end{equation}`

- Shortly: it is the expectation of prediction error across possible training sets (from the same distribution).

---

- Note that estimates of the quantities `\(Err_{XY}\)` and `\(Err\)` **cannot be computed** when the **data distribution** `\(P\)` is **unknown**.
- Then, &lt;b&gt;&lt;span style="color:#067373;"&gt;resampling based methods&lt;/span&gt;&lt;/b&gt; such as cross-validation, bootstrap, and jacknife or &lt;b&gt;&lt;span style="color:#067373;"&gt;analytical methods&lt;/span&gt;&lt;/b&gt; such as AIC, BIC, Mallow's `\(C_p\)`, and covariance penalties can be used to **estimate the quantities** `\(Err_{XY}\)` and `\(Err\)`.


---
# K-fold cross-validation
&lt;br&gt;
- In &lt;b&gt;&lt;span style="color:#067373;"&gt;K-fold cross-validation&lt;/span&gt;&lt;/b&gt;, we randomly partition the data into equally sized &lt;b&gt;&lt;span style="color:#067373;"&gt;disjoint
folds (subsets)&lt;/span&gt;&lt;/b&gt; `\(\mathcal{I}_k\)` `\((k=1,\ldots,K)\)`, where the fold size is `\(m=n/K\)` and the whole data is `\(\mathcal{I} = \cup_{k=1}^{K} \mathcal{I}_k\)`.
- When the data point `\((x_i,y_i) \in \mathcal{I}_k\)`, we will also write `\(i \in \mathcal{I}_k\)` `\((k=1,\ldots,K)\)`.

&lt;div class="figure" style="text-align: center"&gt;
&lt;img src="imgs/K_CV_single.png" alt="&amp;lt;i&amp;gt;Partition of the data into K-folds.&amp;lt;/i&amp;gt;" width="13%" /&gt;
&lt;p class="caption"&gt;&lt;i&gt;Partition of the data into K-folds.&lt;/i&gt;&lt;/p&gt;
&lt;/div&gt;

&lt;div style="text-align:right;"&gt;&lt;font size="4px"&gt;&lt;a href="https://www.researchgate.net/profile/Mingchao-Li/publication/331209203/figure/fig2/AS:728070977748994@1550597056956/K-fold-cross-validation-method.png"&gt;Image Source&lt;/a&gt;&lt;/font&gt;&lt;/div&gt;

---

- Consider the first fold `\(\mathcal{I}_{1}\)` and hold it out as future data set (or &lt;b&gt;&lt;span style="color:#067373;"&gt;test set&lt;/span&gt;&lt;/b&gt;).
- The data points `\((x_i,y_i) \in \mathcal{I} \backslash \mathcal{I}_{1}\)`, which are not in the first fold, are called as &lt;b&gt;&lt;span style="color:#067373;"&gt;training set&lt;/span&gt;&lt;/b&gt;.
- Let `\(\hat \theta^{(-1)} = \mathcal{A}\big((X_i,Y_i)_{i \in \mathcal{I} \backslash \mathcal{I}_{1}}\big)\)`  be the **model fit on training data set**, then
we calculate the **prediction error** over fixed `\(\mathcal{I}_{1}\)` set as follows:

`\begin{equation}
 \frac{1}{m} \sum_{{i} \in \mathcal{I}_1}\ell \big(\hat{f}(x_{i},\hat \theta^{(-1) } ),y_i\big).
\end{equation}`
---
- In &lt;b&gt;&lt;span style="color:#067373;"&gt;K-fold cross-validation&lt;/span&gt;&lt;/b&gt;, we repeat this process for each fold `\((k=1,\ldots,K)\)`.


&lt;div class="figure" style="text-align: center"&gt;
&lt;img src="imgs/K_CV.png" alt="&amp;lt;i&amp;gt;K-fold cross-validation.&amp;lt;/i&amp;gt;" width="60%" /&gt;
&lt;p class="caption"&gt;&lt;i&gt;K-fold cross-validation.&lt;/i&gt;&lt;/p&gt;
&lt;/div&gt;

&lt;div style="text-align:right;"&gt;&lt;font size="4px"&gt;&lt;a href="https://www.researchgate.net/profile/Mingchao-Li/publication/331209203/figure/fig2/AS:728070977748994@1550597056956/K-fold-cross-validation-method.png"&gt;Image Source&lt;/a&gt;&lt;/font&gt;&lt;/div&gt;

---
# CV estimate of prediction error
&lt;br&gt;
- The **average of errors over K folds** is given as follows:

`\begin{equation}
	\widehat {Err} ^{(CV)} := \frac{1} {K} \sum_{k=1}^{K} \frac{1} {m}\sum_{{i} \in \mathcal{I}_k} \ell \big(\hat{f}(x_{i},\hat \theta^{(-1) } ),y_i\big).
\end{equation}`

- This is usually called as the &lt;b&gt;&lt;span style="color:#067373;"&gt;CV estimate of prediction error&lt;/span&gt;&lt;/b&gt;.
- **Relationship between `\(\widehat {Err} ^{(CV)}\)`, `\(Err_{XY}\)`, and `\(Err\)`**: Intuitively, the inner sum is an estimate for `\(Err_{XY}\)` for a fixed fold, and the double sum estimates `\(Err\)` with `\(\mathcal{I}_k\)`  `\(( k=1,\ldots,K)\)` being different samples from the same distribution &lt;b&gt;&lt;span style="color:#067373;"&gt;(De Benito Delgado, 2021)&lt;/span&gt;&lt;/b&gt;.

---
# What prediction error are we estimating?

&lt;br&gt;
- `\(Err_{XY}\)` is the &lt;b&gt;&lt;span style="color:#067373;"&gt;prediction error of the model which was fit on the training set&lt;/span&gt;&lt;/b&gt;.
- `\(Err\)` is the &lt;b&gt;&lt;span style="color:#067373;"&gt;average of the fitting algorithm run on the same-sized data sets drawn
from the same distribution `\(P\)`.&lt;/span&gt;&lt;/b&gt;
- The former quantity is of the most interest to a practitioner deploying a specific model, whereas the latter  may be of interest to a researcher comparing different fitting algorithms.
---
# `\(Err_{X}\)`:  A different target of inference
&lt;br&gt;
- Assume a &lt;b&gt;&lt;span style="color:#067373;"&gt;homoskedastic Gaussian linear model&lt;/span&gt;&lt;/b&gt; as follows:

`\begin{equation}
y_i = x_{i}^{T} \theta + \epsilon_i \quad \text{where} \quad \epsilon_i \overset{i.i.d.}{\sim} N(0,\sigma^2)  \quad \text{i=1,...,n.}
\end{equation}`

- Define a &lt;b&gt;&lt;span style="color:#067373;"&gt;new key quantity&lt;/span&gt;&lt;/b&gt; as follows:

`\begin{equation}
Err_{X} := \mathbb{E}[Err_{XY} | X],
\end{equation}`

- which &lt;b&gt;&lt;span style="color:#067373;"&gt;falls between Err&lt;/span&gt;&lt;/b&gt; and &lt;b&gt;&lt;span style="color:#067373;"&gt; `\(Err_{XY}\)`&lt;/span&gt;&lt;/b&gt; as visualized below:

&lt;br&gt;
&lt;div class="figure" style="text-align: center"&gt;
&lt;img src="imgs/paper_fig2.png" alt="&amp;lt;i&amp;gt;Possible targets of inference for cross-validation.&amp;lt;/i&amp;gt;" width="60%" /&gt;
&lt;p class="caption"&gt;&lt;i&gt;Possible targets of inference for cross-validation.&lt;/i&gt;&lt;/p&gt;
&lt;/div&gt;

---

$$ \newcommand\independent{\protect\mathpalette{\protect\independenT}{\perp}}
\def\independenT#1#2{\mathrel{\rlap{$#1#2$}\mkern2mu{#1#2}}}$$ 

- **Lemma 1:** When Ordinary Least Squares (OLS) is used as the fitting
algorithm along with a squared-error loss, the CV estimate of prediction error, 
`\(\widehat {Err}^{(CV)}\)` is linearly invariant.

--

- &lt;&lt; The CV estimate of prediction is the same for both the **original data** `\((y_{i})\)` and 
**shifted data** `\((y_{i}^{'} = y_{i} + x_{i}^{T} \kappa)\)` since the residual is the same when using the `\((x_{1}, y_{1}),...,(x_{n}, y_{n})\)` or the shifted data `\((x_{1}, y_{1}^{'}),...,(x_{n}, y_{n}^{'})\)`. &gt;&gt;

---
&lt;br&gt;
- **Theorem 1:** Assume homoskedastic Gaussian linear model holds and that we use
squared-error loss. Let `\(\widehat {Err}\)` be a **linearly invariant estimate of prediction
error** (such as `\(\widehat {Err}^{(CV)}\)` using OLS as the fitting algorithm). Then,

`\begin{equation}
Err_{XY} \perp \widehat {Err} \quad | \quad X.
\end{equation}`

--
- &lt;&lt; Recall from classical linear regression theory that when using OLS, the **estimated coefficient** vector `\(\widehat {\theta}\)` is independent of the **residuals** `\((Y-X\widehat {\theta})\)`:

`\begin{equation}
\widehat {\theta} \perp (Y-X\widehat {\theta}) \quad | \quad X.
\end{equation}`

- Since `\(Err_{XY}\)`  is a function of only of `\(\widehat {\theta}\)`, which the OLS estimate of `\(\theta\)` based on full sample, and any linearly invariant estimate of prediction error `\(Err\)`  is a function only of residuals, `\(Y-X\widehat {\theta}\)`, by the invariance property, then `\(Err_{XY} \perp \widehat {Err} \quad | \quad X\)`. &gt;&gt;

---
&lt;br&gt;
- **Remark 1:** Suppose we have two data sets `\((X,Y)\)` and `\((X, Y^{'})\)` that share the same feature matrix `\(X\)`. Let `\(Err_{XY}\)` and `\(Err_{XY^{'}}\)` be true errors of the model fit with these two data sets, respectively. Next, suppose we perform cross-validation on `\((X,Y)\)` to get an estimate `\(Err_{XY}\)` and do the same on `\((X, Y^{'})\)` to get an estimate `\(Err_{XY^{'}}\)`. Theorem 1 implies that

`\begin{equation}
(Err_{XY}, \widehat Err_{XY}^{CV}) \overset{d}{=}  (Err_{XY}, \widehat Err_{XY^{'}}^{CV}).
\end{equation}`

--

- &lt;&lt; This means that for the purpose of estimating `\(Err_{XY}\)`, we have no reason to prefer using the cross-validation estimate with
`\((X,Y)\)` to using the cross-validation estimate with a different data set `\((X, Y^{'})\)`, even though we wish to estimate the error of the model fit on `\((X,Y)\)`. &gt;&gt;

---
- **Corollary 1:**  Under the conditions of Theorem 1,

`\begin{equation}
\mathbb{E}\Big[\big(\widehat Err-Err_{XY}\big)^2\Big] = \mathbb{E}\Big[\big(\widehat Err-Err_{X}\big)^2\Big] + \mathbb{E}\Big[\big( Err_{X}-Err_{XY}\big)^2\Big].
\end{equation}`

--

- &lt;&lt; Any linearly invariant estimator (such as cross-validation `\(Err^{CV}\)`) has larger mean squared error as an estimate of `\(Err_{XY}\)` than as an estimate of `\(Err_{X}\)`. &gt;&gt;

- This implies that `\(\widehat Err^{CV}\)` is a better estimate of an intermediate quantity `\(Err_{X}\)` than of `\(Err_{XY}\)`.

---
# Example
&lt;br&gt;
- Consider an experiment in a simple linear model with `\(n=100\)` observations
and `\(p=20\)` features `\(\overset{i.i.d.}{\sim} N(0,1)\)`, which is replicated `\(n=2000\)` times with the same feature matrix `\(X\)`.

&lt;img src="imgs/paper_fig3.png" width="60%" style="display: block; margin: auto;" /&gt;

- Left: MSE of CV point estimate of prediction relative to the estimands: `\(Err\)`, `\(Err_{X}\)`, and `\(Err_{XY}\)`.
- Center: Coverage of `\(Err\)`, `\(Err_{X}\)`, and `\(Err_{XY}\)` by the naive cross-validation intervals, where nominal miscoverage rate is `\(10\%\)`.

---
- We see that CV point estimate has lower MSE for `\(Err_{X}\)` than `\(Err_{XY}\)`.
- The naive CV intervals cover `\(Err_{X}\)` more often than they cover `\(Err_{XY}\)`.

&lt;br&gt;

&lt;img src="imgs/paper_fig3.png" width="60%" style="display: block; margin: auto;" /&gt;

---
# Relationship between `\(Err\)` and `\(Err_{X}\)`
&lt;br&gt;
- The conditional variance decomposition of the variance of `\(Err_{XY}\)`:

`\begin{align}
Var(Err_{XY}) &amp;=\mathbb{E}_{X}\big[Var(Err_{XY}|X)\big] + Var\big(\mathbb{E}_{X}[Err_{XY}|X]\big) \\
              &amp;=\mathbb{E}_{X}\big[Var(Err_{XY}|X)\big] + Var\big(Err_{X}\big),
\end{align}`

- `\(Var\big(Err_{X}\big)\)`: is the variance caused by the randomness in `\(X\)` and
- `\(\mathbb{E}_{X}\big[Var(Err_{XY}|X)\big]\)`: is the variance caused by the randomness in `\(Y | X\)`.



---
# Relationship between `\(Err\)` and `\(Err_{X}\)`
&lt;br&gt;
- **Theorem 2:** Suppose the homeskedastic Gaussian linear model holds and that we use a squared-error loss. In addition, assume that feature vectors `\(X \sim N(0,\Sigma_p)\)` for any
full-rank `\(\Sigma_p\)`. Then, for `\(n &gt; p\)`, as `\(n, p  \rightarrow \infty\)` with `\(n/p \rightarrow \lambda &gt;1\)`, we have:


`\begin{align}
\mathbb{E}_{X}\big[Var(Err_{XY}|X)\big] = \Theta(1/n) \quad \text{and} \\
Var\big(Err_{X}\big) = \mathbb{E}(Err_{X}-Err)^2 = \Theta(1/n^2).
\end{align}`

- We see that the randomness caused by `\(Y\)` given `\(X\)` is of larger order than that due the randomness in `\(X\)`.
---
- The variance of `\(Err_{X}\)` (which also has mean `\(Err\)`) is small compared with the variance of `\(Err_{XY}\)` (which also has mean `\(Err\)`), showing that `\(Err_{X}\)` is close to `\(Err\)`.
- Note that MSE and coverage of cross-validation is similar when estimating either `\(Err\)` or `\(Err_{X}\)`, but significantly different when estimating `\(Err_{XY}\)`.

&lt;img src="imgs/paper_fig3.png" width="60%" style="display: block; margin: auto;" /&gt;

---
- In the proportional asymptotic limit, implies that `\(	\widehat {Err} ^{(CV)}\)`
is closer to `\(Err\)` and `\(Err_{X}\)` than to `\(Err_{XY}\)`.

- **Corollary 2:** In the setting of Theorem 2,

`\begin{align}
cor(Err_{XY}, Err_{X}) \rightarrow 0, \quad \text{n, p} \rightarrow \infty.
\end{align}`

- Moreover, for any linearly invariant estimator `\(\widehat {Err}\)` (such as `\(	\widehat {Err} ^{(CV)}\)` using OLS as the fitting algorithm), 

`\begin{align}
cor(Err_{XY}, \widehat{Err}) \rightarrow 0, \quad \text{n, p} \rightarrow \infty.
\end{align}`

- This implies that `\(	\widehat{Err}^{(CV)}\)` is asymptotically uncorrelated with `\(Err_{XY}\)`.

---
- A note: The theory developed in Sections 2 and 3 of the paper applies not only to CV but to other methods as well: data-splitting with refitting (train on one half of the data, evaluate on the other,
then refit on the whole data set), Mallow's `\(C_p\)` and bootstrap.

---
# Dependence structure of CV errors
- Let 
`\(e_i =\ell \big(\hat{f}(x_{i},\hat \theta^{(-1) } ),y_i\big)\)` for each `\(i \in \mathcal{I}_k\)` ( `\(k=1,\ldots,K\)` ), and the errors `\(e_i\)` for points in other folds are defined
similarly.
- Since `\(n =  K \times m\)`,  we can **re-define** &lt;b&gt;&lt;span style="color:#067373;"&gt;CV estimate of prediction error&lt;/span&gt;&lt;/b&gt; as follows:

`\begin{equation}
	\widehat {Err } ^{(CV)}:= 	\bar e = \frac{1} {n}  \sum_{i=1}^{n}e_i. \nonumber
\end{equation}`

- Remember that
`\(e_i =\ell \big(\hat{f}(x_{i},\hat \theta^{(-1) } ),y_i\big)\)` for each `\(i \in \mathcal{I}_k\)` ( `\(k=1,\ldots,K\)` ), and the errors `\(e_i\)` for points in other folds are defined
similarly.

---
- The cross-validation estimate of the prediction error is:

`\begin{equation}
	\widehat {Err } ^{(CV)}:= 	\bar e = \frac{1} {n}  \sum_{i=1}^{n}e_i. \nonumber
\end{equation}`

- Assuming that `\(e_i\)`'s are i.i.d., an &lt;b&gt;&lt;span style="color:#067373;"&gt;estimate of the standard error of the prediction error&lt;/span&gt;&lt;/b&gt; would be:

`\begin{equation}
	\widehat{se}^{(CV)}:=\frac{1} {\sqrt n} \times \sqrt {\frac{1} {n-1} \sum_{i=1}^{n}(e_i-\bar e)^2},  \nonumber
\end{equation}`
- where the second term in the multiplication refers to the  &lt;b&gt;&lt;span style="color:#067373;"&gt;empirical standard deviation of the `\(e_i\)`&lt;/span&gt;&lt;/b&gt;. 

---
# A `\(100(1-\alpha)\%\)` confidence interval for prediction error
&lt;br&gt;
- A `\(100(1-\alpha)\%\)` &lt;b&gt;&lt;span style="color:#067373;"&gt;confidence interval for prediction error&lt;/span&gt;&lt;/b&gt; can be constructed as follows:

`\begin{equation}
\Big(\widehat {Err}^{(CV)} - z_{1-(\frac{\alpha}{2})} \times \widehat{se}^{(CV)} \quad, \quad \widehat {Err}^{(CV)} + z_{1-(\frac{\alpha}{2})} \times \widehat{se}^{(CV)} \Big)
\end{equation}`

- where `\(z_{1- (\frac{\alpha}{2}) }\)` is the `\(1- (\frac{\alpha}{2})\)` quantile of the standard normal distribution. 
- The intervals are called as &lt;b&gt;&lt;span style="color:#067373;"&gt;naive cross-validation intervals&lt;/span&gt;&lt;/b&gt;.

- However, since every data point is used in both in training and testing, we cannot accept that are `\(e_i\)`'s are independent of each other. 

- As K approaches to n, correlations between `\(e_i\)`'s would increase, and hence would results in higher variance.

- Any confidence interval built using will be too small and have poor coverage.

---
# Example

- Consider a sparse logistic model:

`\begin{align}
Pr(Y_i = 1 | X_i = x_i) = \frac{1}{1 + exp(-x_{i}^{T}\theta)}, \quad \text{i=1,...,n},
\end{align}`

- where `\(n=90\)` observations of `\(p=1000\)` features, and a coefficient 
`\(\theta = c \times (1,1,1,1,0,0,...)^{T} \in \mathcal{R}^{p}\)` and `\(c\)` is chosen such that misclassification rate is `\(20 \%\)`.

- We estimate the parameters using `\(\ell_1-\)` penalized logistic regression with a fixed
penalty level.

---
&lt;br&gt;
- In this case, naive confidence intervals for prediction error are far too small: intervals with desired miscoverage of `\(10\%\)` give `\(31\%\)` miscoverage in our simulation.

&lt;div class="figure" style="text-align: center"&gt;
&lt;img src="imgs/logistic_cv.png" alt="&amp;lt;i&amp;gt;Structure of the covariance matrix of errrors.&amp;lt;/i&amp;gt;" width="45%" /&gt;
&lt;p class="caption"&gt;&lt;i&gt;Structure of the covariance matrix of errrors.&lt;/i&gt;&lt;/p&gt;
&lt;/div&gt;

---
&lt;style type="text/css"&gt;
.pull-left {
  float: left;
  width: 50%;
}
.pull-right {
  float: right;
  width: 50%;
}
&lt;/style&gt;

.pull-left[
- The fundamental paper of Bengio and Grandvalet (2004) gives the key structure of the covariance matrix of `\(e_i\)`'s such that:

`\begin{equation}
Var(\widehat {Err}^{(CV)}) = \frac{1}{n}a_1 + \frac{n/K-1}{n}a_2 + \frac{n-n/K}{n}a_3,
\end{equation}`

- where `\(a_1=Var(e_i)\)` is the variance of the diagonal elements,
- `\(a_2 = Cov(e_i, e_j)\)` is the covariance of the off-diagonal elements within the same fold (in-block covariance of errors due to a common training set), and
- `\(a_3 = Cov(e_i, e_j)\)`  is the covariance between-blocks covariance due the dependence between training sets `\(\mathcal{I}_k\)`  `\((k=1,\ldots,K)\)`.



]


.pull-right[
&lt;div class="figure" style="text-align: center"&gt;
&lt;img src="imgs/covariance_structure.png" alt="&amp;lt;i&amp;gt;Structure of the covariance matrix of errrors.&amp;lt;/i&amp;gt;" width="70%" /&gt;
&lt;p class="caption"&gt;&lt;i&gt;Structure of the covariance matrix of errrors.&lt;/i&gt;&lt;/p&gt;
&lt;/div&gt;

&lt;div style="text-align:right;"&gt;&lt;font size="4px"&gt;&lt;a href="https://www.jmlr.org/papers/volume5/grandvalet04a/grandvalet04a.pdf"&gt;Image Source&lt;/a&gt;&lt;/font&gt;&lt;/div&gt;
]

---
- The constants `\(a_2\)` and `\(a_3\)` will typically be positive, in which case 

`\begin{equation}
Var(\widehat {Err}^{(CV)}) &gt; \frac{1}{n}a_1,
\end{equation}`

- so estimating the variance of `\(\widehat{Err}^{(CV)}\)` as `\(\widehat{se}^2\)` results in an estimate that it is too small, and, in turn, poor coverage.


---
# Target of inference

- **Definition 2:** For a sample of size n split into K folds, the cross-validation MSE is:

`\begin{equation}
MSE_{K,n} := \mathbb{E}\big[ (\widehat {Err}^{(CV)} - Err_{XY})^2 \big]
\end{equation}`

- Due to technical reasons, MSE is defined with respect to  `\(Err_{XY}\)`
- MSE contains both a bias term and variance term, but the bias typically
small for cross-validation [Efron, 1983; Efron and Gong, 1983; Efron and Tibshirani, 1997].
- We can view the MSE as a slightly conservative version of the variance of the cross-validation estimator, i.e., the squared standard error.
- We will use an estimate of the MSE to construct confidence intervals for `\(Err_{XY}\)` since it is what
typically matters for practical applications.
- With this in mind, we will use an estimate of the MSE to construct confidence intervals for `\(Err_{XY}\)`. 

---
# Algorithm: Nested cross validaion

- Split D into K folds with K-1 building `\(D_{train}\)` and the remaining
one being `\(D_{out}\)`, with indices `\(I_{out}\)`.
- For each split j:
  - Compute `\(\epsilon_j := \bar{e}_{in}=\widehat {Err}_{\tilde X \tilde Y}\)` with (K-2)-fold
  CV over K-1 folds in `\(D_{in}\)`.
  - Train model on `\(D_{train}\)`. Compute `\(e_{i}\)` for all samples `\((x_i,y_i) \in D_{out}\)`.
  - Compute `\(\bar e_{out} := \text{mean of} \quad \{e_i\}_{e \in I_{out}}\)`
  - Set `\(a_j := (\widehat {Err}_{\tilde X \tilde Y} - \bar e_{out})^2\)` (estimate of (a)).
  - Set `\(b_j := \text{empirical variance of} \quad \{e_i\}_{e \in I_{out}}\)` (estimate of (b)).
- Output  `\(\widehat{MSE} = mean(a_j)-mean(b_j)\)`. 
- Output `\(\widehat {Err}^{(NCV)} := mean(\epsilon_j)\)`.
 


---
# Estimation of bias

`\begin{equation}
\widehat {Err}^{(NCV)} - \widehat {Err}^{(CV)}
\end{equation}`


`\begin{equation}
\widehat {bias} := \bigg(1 + \Big(\frac{K-2}{K}\Big)\bigg) (\widehat {Err}^{(NCV)} - \widehat {Err}^{(CV)})
\end{equation}`


`\begin{equation}
\Big(\widehat {Err}^{(NCV)} - \widehat {bias}- z_{1-(\frac{\alpha}{2})} \sqrt{\frac{K-1}{K}}\sqrt{\widehat {MSE}}, \widehat {Err}^{(NCV)} + \widehat {bias}- z_{1-(\frac{\alpha}{2})} \sqrt{\frac{K-1}{K}}\sqrt{\widehat {MSE}}\Big)
\end{equation}`

---
# Future work

- It is well-known that CV is also commonly used for selecting a good value of a learning algorithm's hyperparameters (fine-tuning).
- &lt;b&gt;&lt;span style="color:#067373;"&gt;Bates, Hastie, and Tibshirani (2021)&lt;/span&gt;&lt;/b&gt; expect that NCV would be of use for hyperparameter selection since it yields more accurate confidence intervals for prediction error.

---

class: center, middle



&lt;br&gt; Merci!.. &lt;/br&gt;
    </textarea>
<style data-target="print-only">@media screen {.remark-slide-container{display:block;}.remark-slide-scaler{box-shadow:none;}}</style>
<script src="https://remarkjs.com/downloads/remark-latest.min.js"></script>
<script>var slideshow = remark.create({
"highlightStyle": "atelier-forest-light",
"highlightLines": true,
"highlightSpans": true,
"countIncrementalSlides": true,
"ratio": "16:9"
});
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {
  window.dispatchEvent(new Event('resize'));
});
(function(d) {
  var s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler");
  if (!r) return;
  s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }";
  d.head.appendChild(s);
})(document);

(function(d) {
  var el = d.getElementsByClassName("remark-slides-area");
  if (!el) return;
  var slide, slides = slideshow.getSlides(), els = el[0].children;
  for (var i = 1; i < slides.length; i++) {
    slide = slides[i];
    if (slide.properties.continued === "true" || slide.properties.count === "false") {
      els[i - 1].className += ' has-continuation';
    }
  }
  var s = d.createElement("style");
  s.type = "text/css"; s.innerHTML = "@media print { .has-continuation { display: none; } }";
  d.head.appendChild(s);
})(document);
// delete the temporary CSS (for displaying all slides initially) when the user
// starts to view slides
(function() {
  var deleted = false;
  slideshow.on('beforeShowSlide', function(slide) {
    if (deleted) return;
    var sheets = document.styleSheets, node;
    for (var i = 0; i < sheets.length; i++) {
      node = sheets[i].ownerNode;
      if (node.dataset["target"] !== "print-only") continue;
      node.parentNode.removeChild(node);
    }
    deleted = true;
  });
})();
(function() {
  "use strict"
  // Replace <script> tags in slides area to make them executable
  var scripts = document.querySelectorAll(
    '.remark-slides-area .remark-slide-container script'
  );
  if (!scripts.length) return;
  for (var i = 0; i < scripts.length; i++) {
    var s = document.createElement('script');
    var code = document.createTextNode(scripts[i].textContent);
    s.appendChild(code);
    var scriptAttrs = scripts[i].attributes;
    for (var j = 0; j < scriptAttrs.length; j++) {
      s.setAttribute(scriptAttrs[j].name, scriptAttrs[j].value);
    }
    scripts[i].parentElement.replaceChild(s, scripts[i]);
  }
})();
(function() {
  var links = document.getElementsByTagName('a');
  for (var i = 0; i < links.length; i++) {
    if (/^(https?:)?\/\//.test(links[i].getAttribute('href'))) {
      links[i].target = '_blank';
    }
  }
})();
// adds .remark-code-has-line-highlighted class to <pre> parent elements
// of code chunks containing highlighted lines with class .remark-code-line-highlighted
(function(d) {
  const hlines = d.querySelectorAll('.remark-code-line-highlighted');
  const preParents = [];
  const findPreParent = function(line, p = 0) {
    if (p > 1) return null; // traverse up no further than grandparent
    const el = line.parentElement;
    return el.tagName === "PRE" ? el : findPreParent(el, ++p);
  };

  for (let line of hlines) {
    let pre = findPreParent(line);
    if (pre && !preParents.includes(pre)) preParents.push(pre);
  }
  preParents.forEach(p => p.classList.add("remark-code-has-line-highlighted"));
})(document);</script>

<script>
slideshow._releaseMath = function(el) {
  var i, text, code, codes = el.getElementsByTagName('code');
  for (i = 0; i < codes.length;) {
    code = codes[i];
    if (code.parentNode.tagName !== 'PRE' && code.childElementCount === 0) {
      text = code.textContent;
      if (/^\\\((.|\s)+\\\)$/.test(text) || /^\\\[(.|\s)+\\\]$/.test(text) ||
          /^\$\$(.|\s)+\$\$$/.test(text) ||
          /^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(text)) {
        code.outerHTML = code.innerHTML;  // remove <code></code>
        continue;
      }
    }
    i++;
  }
};
slideshow._releaseMath(document);
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>
